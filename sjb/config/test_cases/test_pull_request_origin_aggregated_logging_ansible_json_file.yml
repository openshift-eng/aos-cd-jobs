---
parent: 'common/test_cases/origin_built_release.yml'
overrides:
  sync_repos:
    - name: "origin-aggregated-logging"
    - name: "openshift-ansible"
    - name: "aos-cd-jobs"
extensions:
  actions:
    - type: "script"
      title: "build an origin-aggregated-logging release"
      repository: "origin-aggregated-logging"
      script: |-
        hack/build-images.sh
    - type: "script"
      title: "install Ansible and other packages needed for OpenShift Ansible tasks"
      repository: "origin-aggregated-logging"
      script: |-
        sudo yum install -y ansible python2 python-six tar java-1.8.0-openjdk-headless httpd-tools \
           libselinux-python python-passlib python-pip
    - type: "script"
      title: "install Ansible plugins"
      repository: "origin-aggregated-logging"
      script: |-
        sudo chmod o+rw /etc/environment
        echo "ANSIBLE_JUNIT_DIR=$( pwd )/_output/scripts/ansible_junit" >> /etc/environment
        sudo mkdir -p /usr/share/ansible/plugins/callback
        for plugin in 'default_with_output_lists' 'generate_junit'; do
           wget "https://raw.githubusercontent.com/openshift/origin-ci-tool/master/oct/ansible/oct/callback_plugins/${plugin}.py"
           sudo mv "${plugin}.py" /usr/share/ansible/plugins/callback
        done
        sudo sed -r -i -e 's/^#?stdout_callback.*/stdout_callback = default_with_output_lists/' -e 's/^#?callback_whitelist.*/callback_whitelist = generate_junit/' /etc/ansible/ansible.cfg
    - type: "script"
      title: "ensure the correct package repositories are used for the correct release"
      repository: "origin-aggregated-logging"
      script: |-
        # is logging using master or a release branch?
        curbranch=$( git rev-parse --abbrev-ref HEAD )
        if [[ "${curbranch}" == master ]] ; then
           # enable centos-openshift-origin and centos-openshift-origin-testing
           # assume everything else has been disabled
           sudo yum-config-manager --enable centos-openshift-origin > /dev/null
           sudo yum-config-manager --enable centos-openshift-origin-testing > /dev/null
        elif [[ "${curbranch}" =~ ^release-* ]] ; then
           # get repo ver from branch name
           repover=$( echo "${curbranch}" | sed -e 's/release-//' -e 's/[.]//' )
           # get version from tag
           closest_tag=$( git describe --tags --abbrev=0 )
           # pkg ver is closest_tag with leading "-" instead of "v"
           pkgver=$( echo "${closest_tag}" | sed 's/^v/-/' )
           # disable all of the centos repos except for the one for the
           # version being tested - this assumes a devenv environment where
           # all of the repos are installed
           for repo in $( sudo yum repolist all | awk '/^[!]?centos-paas-sig-openshift-origin/ {print gensub(/^!/,"",1,$1)}' ) ; do
              case $repo in
                 centos-paas-sig-openshift-origin${repover}-rpms) sudo yum-config-manager --enable $repo > /dev/null ;;
                 *) sudo yum-config-manager --disable $repo > /dev/null ;;
              esac
           done
           sudo yum-config-manager --disable origin-local-release > /dev/null
        else
           echo Error: unknown base branch $curbranch: please resubmit PR on master or a release-x.y branch
        fi
    - type: "script"
      title: "configure system to run oc cluster up and run it"
      repository: "origin-aggregated-logging"
      script: |-
        # https://github.com/openshift/origin/blob/master/docs/cluster_up_down.md#overview
        # 3.8 has a problem with fail-swap-on - not a valid flag
        sudo yum install -y origin-clients docker /usr/bin/firewall-cmd
        sudo yum info origin-clients
        sudo systemctl enable firewalld
        sudo systemctl start firewalld
        sudo sysctl -w net.ipv4.ip_forward=1

        # set up docker for local registry
        if ! sudo grep -q '^INSECURE_REGISTRY=' /etc/sysconfig/docker ; then
          # not present - easy - just add it
          echo "INSECURE_REGISTRY='--insecure-registry=172.30.0.0/16'" | sudo tee -a /etc/sysconfig/docker > /dev/null
        elif ! sudo grep -q '^INSECURE_REGISTRY=.*--insecure-registry=172.30.0.0/16' /etc/sysconfig/docker ; then
          # already there - just add our registry - assumes 1 single line
          sudo sed -e "/^INSECURE_REGISTRY=/s,[']$, --insecure-registry 172.30.0.0/16'," -i /etc/sysconfig/docker
        fi

        # set docker log driver correctly
        if ! sudo grep -q '^OPTIONS=' /etc/sysconfig/docker ; then
          # not present - easy - just add it
          echo "OPTIONS='--log-driver=${LOG_DRIVER:-json-file}'" | sudo tee -a /etc/sysconfig/docker > /dev/null
        elif ! sudo grep -q '^OPTIONS=.*--log-driver' /etc/sysconfig/docker ; then
          # already there - just add our value - assumes 1 single line
          sudo sed -e "/^OPTIONS=/s,[']$, --log-driver=${LOG_DRIVER:-json-file}'," -i /etc/sysconfig/docker
        else
          # already there with value - change the value
          sudo sed -e "/^OPTIONS=/s/--log-driver=[-_a-zA-Z0-9][-_a-zA-Z0-9]*/--log-driver=${LOG_DRIVER:-json-file}/" -i /etc/sysconfig/docker
        fi

        sudo systemctl daemon-reload
        sudo systemctl restart docker

        subnet=$( sudo docker network inspect -f "{{range .IPAM.Config }}{{ .Subnet }}{{end}}" bridge )
        sudo firewall-cmd --permanent --new-zone dockerc
        sudo firewall-cmd --permanent --zone dockerc --add-source $subnet
        sudo firewall-cmd --permanent --zone dockerc --add-port 8443/tcp
        sudo firewall-cmd --permanent --zone dockerc --add-port 53/udp
        sudo firewall-cmd --permanent --zone dockerc --add-port 8053/udp
        sudo firewall-cmd --reload
        # this is specific to AWS - other platforms will need to get public ip and hostname elsewhere
        metadata_endpoint="http://169.254.169.254/latest/meta-data"
        public_hostname="$( curl -s "${metadata_endpoint}/public-hostname" )"
        public_ip="$( curl -s "${metadata_endpoint}/public-ipv4" )"
        sudo oc cluster up --public-hostname="${public_hostname}" --routing-suffix="${public_ip}.xip.io"
        # change the config
        # allow externalIPs, and kibana UI access
        SERVER_CONFIG_DIR=/var/lib/origin/openshift.local.config
        # add loggingPublicURL so the OpenShift UI Console will include a link for Kibana
        # this part stolen from util.sh configure_os_server()
        sudo cp ${SERVER_CONFIG_DIR}/master/master-config.yaml ${SERVER_CONFIG_DIR}/master/master-config.orig.yaml
        ocver=$( oc version | awk -F'[ v.-]+' '/^oc / {print $2 $3 $4}' )
        patchcmd="sudo oc ex config patch"
        if [ $ocver -lt 380 ] ; then
            patchcmd="docker exec origin openshift ex config patch"
        fi
        $patchcmd ${SERVER_CONFIG_DIR}/master/master-config.orig.yaml \
            --patch="{\"assetConfig\": {\"loggingPublicURL\": \"https://kibana.${public_ip}.xip.io\"}}" | \
            sudo tee ${SERVER_CONFIG_DIR}/master/master-config.yaml > /dev/null
        sudo cp ${SERVER_CONFIG_DIR}/master/master-config.yaml ${SERVER_CONFIG_DIR}/master/master-config.save.yaml
        $patchcmd ${SERVER_CONFIG_DIR}/master/master-config.save.yaml \
            --patch="{\"networkConfig\": {\"externalIPNetworkCIDRs\": [\"0.0.0.0/0\"]}}" | \
            sudo tee ${SERVER_CONFIG_DIR}/master/master-config.yaml > /dev/null
        # restart cluster so changes will take effect
        sudo oc cluster down
        # have to restart docker while oc cluster is down, after it has been initialized, or inter-pod
        # networking does not work
        sudo systemctl restart docker
        sudo oc cluster up --use-existing-config --public-hostname="${public_hostname}" --routing-suffix="${public_ip}.xip.io"
    - type: "script"
      title: "expose the kubeconfig"
      script: |-
        if [ ! -d $HOME/.kube ] ; then
          mkdir $HOME/.kube
        fi
        sudo cp /var/lib/origin/openshift.local.config/master/admin.kubeconfig $HOME/.kube/config
        sudo chown $USER $HOME/.kube/config
        sudo mkdir -p /etc/origin/master
        sudo cp /var/lib/origin/openshift.local.config/master/admin.kubeconfig /etc/origin/master/admin.kubeconfig
        sudo chmod a+x /etc/ /etc/origin/ /etc/origin/master/
        sudo chmod a+rw /etc/origin/master/admin.kubeconfig
    - type: "script"
      title: "install origin-aggregated-logging"
      repository: "openshift-ansible"
      script: |-
        OS_A_C_J_DIR=/data/src/github.com/openshift/aos-cd-jobs
        pushd $OS_A_C_J_DIR > /dev/null
        # cannot use the networking related parameters with cluster up deployment
        if [ -f sjb/inventory/group_vars/OSEv3/general.yml ] ; then
            sed -e '/^osm_cluster_network_cidr/d' \
                -e '/^openshift_portal_net/d' \
                -e '/^osm_host_subnet_length/d' \
                -i sjb/inventory/group_vars/OSEv3/general.yml
        else
            echo ERROR: no such file $OS_A_C_J_DIR/sjb/inventory/group_vars/OSEv3/general.yml
            exit 1
        fi
        popd > /dev/null
        # this is specific to AWS - other platforms will need to get public ip and hostname elsewhere
        metadata_endpoint="http://169.254.169.254/latest/meta-data"
        public_hostname="$( curl -s "${metadata_endpoint}/public-hostname" )"
        public_ip="$( curl -s "${metadata_endpoint}/public-ipv4" )"
        playbook_base='playbooks/'
        if [[ -s "${playbook_base}openshift-logging/config.yml" ]]; then
            playbook="${playbook_base}openshift-logging/config.yml"
        else
            playbook="${playbook_base}byo/openshift-cluster/openshift-logging.yml"
        fi
        # richm 20171205 - causes this:
        # RuntimeError: maximum recursion depth exceeded in cmp
        # -e openshift_logging_install_eventrouter=True
        ansible-playbook -vv --become               \
                         --become-user root         \
                         --connection local         \
                         --inventory $OS_A_C_J_DIR/sjb/inventory/ \
                         -e deployment_type=origin  \
                         -e debug_level=2           \
                         -e openshift_logging_install_logging=True \
                         -e openshift_logging_image_prefix="openshift/origin-" \
                         -e openshift_logging_elasticsearch_proxy_image_prefix="docker.io/openshift/" \
                         -e openshift_hosted_logging_hostname="kibana.${public_ip}.xip.io"            \
                         -e openshift_logging_kibana_ops_hostname="kibana-ops.${public_ip}.xip.io"    \
                         -e openshift_logging_master_public_url="https://${public_hostname}:8443"     \
                         -e openshift_master_logging_public_url="https://kibana.${public_ip}.xip.io"  \
                         -e openshift_logging_use_mux=True                                        \
                         -e openshift_logging_mux_allow_external=True                             \
                         -e openshift_logging_es_allow_external=True                              \
                         -e openshift_logging_es_ops_allow_external=True                          \
                         ${playbook} \
                         --skip-tags=update_master_config
    - type: "script"
      title: "run logging tests"
      repository: "origin-aggregated-logging"
      script: |-
        KUBECONFIG=/etc/origin/master/admin.kubeconfig TEST_ONLY=true SKIP_TEARDOWN=true JUNIT_REPORT=true make test
  system_journals:
    - origin-master.service
    - origin-master-api.service
    - origin-master-controllers.service
    - origin-node.service
    - openvswitch.service
    - ovs-vswitchd.service
    - ovsdb-server.service
    - etcd.service
    - systemd-journald.service
  artifacts:
    - "/data/src/github.com/openshift/origin-aggregated-logging/_output/scripts"
