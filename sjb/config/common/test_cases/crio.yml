---
parameters:
  - name: BUILD_ID
    description: "Unique build number for each run."
  - name: REPO_OWNER
    description: "GitHub org that triggered the job."
  - name: REPO_NAME
    description: "GitHub repo that triggered the job."
  - name: PULL_BASE_REF
    description: "Ref name of the base branch."
  - name: PULL_BASE_SHA
    description: "Git SHA of the base branch."
  - name: PULL_REFS
    description: "All refs to test."
  - name: PULL_NUMBER
    description: "Pull request number."
  - name: PULL_PULL_SHA
    description: "Pull request head SHA."
  - name: JOB_SPEC
    description: "JSON form of job specification."
  - name: PROW_JOB_ID
    description: "The ID that prow sets on a Jenkins job in order to correlate it with a ProwJob."
actions:
  - type: "forward_parameters"
    parameters:
    - BUILD_ID
    - REPO_OWNER
    - REPO_NAME
    - PULL_BASE_REF
    - PULL_BASE_SHA
    - PULL_REFS
    - PULL_NUMBER
    - PULL_PULL_SHA
    - JOB_SPEC
    - PROW_JOB_ID
    - JOB_SPEC
  - type: "host_script"
    title: "upload GCS starting metadata"
    timeout: 300
    script: |-
      trap 'exit 0' EXIT
      mkdir -p gcs/
      if [[ -n "${REPO_OWNER:-}" ]]; then
        cat <<STARTED >gcs/started.json
        {
          "timestamp": $( date +%s ),
          "pull": "${PULL_REFS}",
          "repos": {
            "${REPO_OWNER}/${REPO_NAME}": "${PULL_REFS}"
          }
        }
      STARTED
      else
        cat <<STARTED >gcs/started.json
        {
          "timestamp": $( date +%s )
        }
      STARTED
      fi

      BUILD="${BUILD_NUMBER}"

      function gcs_path() {
        bucket="gs://origin-federated-results/"

        suffix="${JOB_NAME}/${BUILD}/"
        if [[ -n "${REPO_OWNER:-}" ]]; then
          if [[ "${REPO_NAME}" != "origin" ]]; then
            segment="${REPO_OWNER}_${REPO_NAME}/"
          fi
          if [[ -n "${PULL_NUMBER:-}" ]]; then
            # this is a presubmit
            prefix="pr-logs/pull/"
            segment="${segment:-}${PULL_NUMBER}/"
          else
            if [[ "${PULL_REFS}" =~ ^[^:]+:[^:]+(,[^:]+:[^:]+){2,} ]]; then
              # this is a batch
              prefix="pr-logs/pull/batch/"
            else
              # this is a postsubmit
              prefix="logs/"
            fi
          fi
        else
          # this is a periodic
          prefix="logs/"
        fi
        echo "${bucket}${prefix}${segment:-}${suffix}"
        return 0
      }
      if path="$( gcs_path )"; then
        gsutil cp gcs/started.json "${path}started.json"
        if [[ -n "${PULL_NUMBER:-}" ]]; then
          echo "${path%\/}" > "${BUILD}.txt"
          gsutil cp "${BUILD}.txt" "gs://origin-federated-results/pr-logs/directory/${JOB_NAME}/${BUILD}.txt"
        fi

        echo ${BUILD} > latest-build.txt
        job_type=$( jq --compact-output ".type" <<<${JOB_SPEC} )
        if [[ $job_type =~ "presubmit" || $job_type =~ "batch" ]]; then
          gsutil cp latest-build.txt "gs://origin-federated-results/pr-logs/directory/${JOB_NAME}/latest-build.txt"
        elif [[ $job_type =~ "postsubmit" || $job_type =~ "periodic" ]]; then
          gsutil cp latest-build.txt "gs://origin-federated-results/logs/${JOB_NAME}/latest-build.txt"
        else
          echo "Bad job type provided in JOB_SPEC: $job_type"
        fi
      fi
post_actions:
  - type: "script"
    title: "create and chwon /data/gcs for artifacts"
    timeout: 300
    script: |-
      sudo mkdir -p /data/gcs
      sudo chown -R origin:origin /data
  - type: "host_script"
    title: "assemble GCS output"
    timeout: 300
    script: |-
      trap 'exit 0' EXIT
      mkdir -p gcs/artifacts gcs/artifacts/generated gcs/artifacts/journals gcs/artifacts/gathered
      result=$( python -c "import json; import urllib; print json.load(urllib.urlopen('${BUILD_URL}api/json'))['result']" )
      cat <<FINISHED >gcs/finished.json
      {
        "timestamp": $( date +%s ),
        "result": "${result}"
      }
      FINISHED
      cat "/var/lib/jenkins/jobs/${JOB_NAME}/builds/${BUILD_NUMBER}/log" > gcs/build-log.txt
      cp -r artifacts/gathered/* gcs/artifacts/ || true
      cp artifacts/generated/* gcs/artifacts/generated/ || true
      cp artifacts/journals/* gcs/artifacts/journals/ || true

      scp -F ${WORKSPACE}/.config/origin-ci-tool/inventory/.ssh_config -r "$( pwd )/gcs" openshiftdevel:/data
      scp -F ${WORKSPACE}/.config/origin-ci-tool/inventory/.ssh_config /var/lib/jenkins/.config/gcloud/gcs-publisher-credentials.json openshiftdevel:/data/credentials.json
  - type: "script"
    title: "push the artifacts and metadata"
    timeout: 300
    script: |-
      trap 'exit 0' EXIT
      if [[ -n "${JOB_SPEC:-}" ]]; then
        JOB_SPEC="$( jq --compact-output ".buildid |= \"${BUILD_NUMBER}\"" <<<"${JOB_SPEC}" )"
        docker run -e JOB_SPEC="${JOB_SPEC}" -v "/data:/data:z" registry.svc.ci.openshift.org/ci/gcsupload:latest --dry-run=false --gcs-path=gs://origin-federated-results --gcs-credentials-file=/data/credentials.json --path-strategy=single --default-org=openshift --default-repo=origin /data/gcs/*
      fi
artifacts:
  - /go/src/k8s.io/kubernetes/artifacts
  - /go/src/k8s.io/kubernetes/e2e.log
  - /go/src/github.com/kubernetes-incubator/cri-o/testout.txt
  - /go/src/github.com/kubernetes-incubator/cri-o/reports
  - /tmp/artifacts
  - /tmp/kubelet.log
  - /tmp/kube-apiserver.log
  - /tmp/kube-controller-manager.log
  - /tmp/kube-proxy.log
  - /tmp/kube-proxy.yaml
  - /tmp/kube-scheduler.log
  - /etc/crio/crio.conf
generated_artifacts:
  installed_packages.log: 'sudo yum list installed'
  avc_denials.log: 'sudo ausearch -m AVC -m SELINUX_ERR -m USER_AVC'
  #filesystem.info: 'sudo df -h && sudo pvs && sudo vgs && sudo lvs'
  # TODO(runcom): sudo df -h hangs because stale nfs mounts are blocking it (straced). Fix it and re-enable sudo df -h here
  filesystem.info: 'sudo pvs && sudo vgs && sudo lvs'
  pid1.journal: 'sudo journalctl _PID=1 --no-pager --all --lines=all'
  system.journal: 'sudo journalctl --no-pager --boot'
system_journals:
  - crio.service
  - customcluster.service
  - systemd-journald.service
